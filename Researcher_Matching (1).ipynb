{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Researcher_Matching.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jd1ZHsTaFSb2"},"source":["#Researcher Matching Solution\n","## Notebook created by Ravi Parashar"]},{"cell_type":"markdown","metadata":{"id":"rF4yHTeWQL5D"},"source":["Here, let us import the libraries necessary for our code to run."]},{"cell_type":"code","metadata":{"id":"kxcog3d7Jjz5"},"source":["from matplotlib import pyplot as plt\n","import pandas as pd\n","import statistics as stat\n","import random\n","from threading import Thread"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nzh33pu4GvyZ"},"source":["## Before diving in...\n","* Can not use a supervised learning approach here\n","  * Do not have labels (orcid) for all instances from which matching could then be done trivially\n","* Semi-supervised approach is not appropriate here either\n","  * We want to use labeled instances to evaluate our approach and not train on them\n","* Problem is not one of classification or regression\n","* The features we have and the solution we want is not appropriate for supervised or semi-supervised classification or regression\n","* Our problem is one of matching alike pairs\n","* Some approach must be created that calculates a similarity score or distance measure between instances\n","* Now, let us take a look at the input data\n","\n","| Column name | Type |\n","| ------------ | --- |\n","| id | Categorical |\n","| first_name | Categorical |\n","| middle_name | Categorical |\n","| lastname | Categorical |\n","| coinvestigator_lastnames/coauthor_lastnames | Variable length categorical array |\n","| topics | Variable length categorical array |\n","| cities | Variable length categorical key-value pair array |\n","| countries | Variable length categorical key-value pair array |\n","| orcid | Categorical |\n","\n","* orcid and id will only be used in evaluation and output of the approaches, respectively (not as inputs to the algorithms) \n","\n","* Here is how we will transform this input data into useful features:\n","  * Each of the elements will be extracted from the variable length categorical arrays\n","  * In the case of the cities and countries arrays, since they are arrays of key-value pairs, only the identifier value will be used since both the identifier and the name represent the same categorical object\n","* The filepaths of the author and investigator csvs are specified below"]},{"cell_type":"code","metadata":{"id":"00rIc8LmqVG6"},"source":["# specify investigator and author csv paths here\r\n","investigator_path = './investigator.csv'\r\n","author_path = './author.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUVvASOuqgXo"},"source":["* The code below will transform the data as specified"]},{"cell_type":"code","metadata":{"id":"WQIZJRvmVxOL"},"source":["# investigators csv read into pandas dataframe\n","df_investigators = pd.read_csv(investigator_path, encoding='latin-1')\n","df_investigators.columns = range(df_investigators.shape[1])\n","inv_ids = []\n","# authors csv read into pandas dataframe\n","df_authors = pd.read_csv(author_path, encoding='latin-1')\n","df_authors.columns = range(df_authors.shape[1])\n","# df_input is concatenation of investigators data to authors data\n","df_input = pd.concat([df_authors, df_investigators], axis=0)\n","df_input = df_input.reset_index()\n","# creating 2d list of coauthor and coinvestigator last names\n","co_names_arr = []\n","for i in df_input[4]:\n","  line_arr = []\n","  if i.find('\\\"') < 0:\n","    co_names_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('\\\"')\n","    while start_ind >= 0:\n","      end_ind = i.find('\\\"', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind+1])\n","      start_ind = i.find('\\\"', end_ind+1)\n","  co_names_arr.append(line_arr)\n","# creating 2d list of topics\n","topics_arr = []\n","for i in df_input[5]:\n","  line_arr = []\n","  if i.find('\\\"') < 0:\n","    topics_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('\\\"')\n","    while start_ind >= 0:\n","      end_ind = i.find('\\\"', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind+1])\n","      start_ind = i.find('\\\"', end_ind+1)\n","  topics_arr.append(line_arr)\n","# creating 2d list of cities\n","cities_arr = []\n","for i in df_input[6]:\n","  line_arr = []\n","  if i[i.find('identifier')+12] == 'n':\n","    cities_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('identifier')+12\n","    while start_ind >= 12:\n","      end_ind = i.find(',', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind])\n","      start_ind = i.find('identifier', end_ind+1)+12\n","  cities_arr.append(line_arr)\n","# creating 2d list of countries\n","countries_arr = []\n","for i in df_input[7]:\n","  line_arr = []\n","  if i[i.find('identifier')+12] == 'n':\n","    countries_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('identifier')+12\n","    while start_ind >= 12:\n","      end_ind = i.find(',', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind])\n","      start_ind = i.find('identifier', end_ind+1)+12\n","  countries_arr.append(line_arr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqVt5E-9dG_2"},"source":["## Exploring the data\n","* An assumption is being made moving forward, that future data (data not included in the dataset) will generally match the formatting in the dataset\n","  * For example, for names, none will be abbreviated (not including initials) and suffixes and prefixes will be excluded\n","* No abnormal data has been seen thus far, proven by the lack of errors in the above code, and thus this is a reasonable assumption\n","* By briefly looking at the data, it seems that the following rules apply to the first name, middle name, and last name of matching instances\n","  * The last name must match exactly\n","  * If the first name is NA or missing for either one, then nothing has to match; If one only has a first initial and the other has the full first name, the first character of the first name must match the other first initial; otherwise, the first name must match exactly\n","  * If the middle name is NA or missing for either one, then nothing has to match; if one only has the middle initial and the other has the full middle name, the first character of the middle name must match the other middle initial; otherwise, the middle name must match exactly\n","* Names which contain periods have their periods removed, as this is not part of the name\n","* Let us look at the matching instances and see if this holds true"]},{"cell_type":"code","metadata":{"id":"XTKDXBCS6KVU"},"source":["investigator_indices = []\n","author_indices = []\n","for i in range(len(df_input[0])):\n","  # the rows in the dataframe which correspond to author data\n","  if i < 1019:\n","    author_indices.append(i)\n","  # the rows in the dataframe which correspond to investigator data\n","  else:\n","    investigator_indices.append(i)\n","for i in investigator_indices:\n","  for a in author_indices:\n","    last = True\n","    middle = True\n","    first = True\n","    # if these are matching instances\n","    if df_input[8][i] == df_input[8][a]:\n","      # if the last names do not exactly match\n","      if not df_input[3][i] == df_input[3][a]:\n","        last = False\n","      # if these are non-NaN values\n","      investigator_first = df_input[1][i]\n","      author_first = df_input[1][a]\n","      if not str(investigator_first) == 'nan' and not str(author_first) == 'nan':\n","        if investigator_first.find('.') >= 0:\n","          investigator_first = investigator_first.replace('.','')\n","        if author_first.find('.') >= 0:\n","          author_first = author_first.replace('.','')\n","        if len(author_first) > 0 and len(investigator_first) > 0:\n","          # if the length of either the investigator or author first name is 1\n","          if len(investigator_first) == 1 or len(author_first) == 1:\n","            # if the first characters of the first names do not match\n","            if not investigator_first[0] == author_first[0]:\n","              first = False\n","          # otherwise\n","          elif len(investigator_first) > 1 and len(author_first) > 1:\n","            # if the first names do not match\n","            if not investigator_first == author_first:\n","              first = False\n","      # if these are non-NaN values\n","      investigator_middle = df_input[2][i]\n","      author_middle = df_input[2][a]\n","      if not str(investigator_middle) == 'nan' and not str(author_middle) == 'nan':\n","        if investigator_middle.find('.') >= 0:\n","          investigator_middle = investigator_middle.replace('.','')\n","        if author_middle.find('.') >= 0:\n","          author_middle = author_middle.replace('.','')\n","        if len(author_middle) > 0 and len(investigator_middle) > 0:\n","          # if the length of either the investigator or author middle name is 1\n","          if len(investigator_middle) == 1 or len(author_middle) == 1:\n","            # if the first characters of the middle names do not match\n","            if not investigator_middle[0] == author_middle[0]:\n","              middle = False\n","          # otherwise\n","          elif len(investigator_middle) > 1 and len(author_middle) > 1:\n","            # if the middle names do not match\n","            if not investigator_middle == author_middle:\n","              middle = False\n","    # if any of the first, middle, or last names do not match as described\n","    # print \"not a match\"\n","    if not first or not middle or not last:\n","      print(\"not a match\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7__dh-GJDnge"},"source":["* Because the above code gives no output, it means that our hypothesis about how names work in matching instances was correct\n","* Let us now move on to seeing how other features relate in matching instances\n","* Firstly, let us examine how often country and city are the same in matching instances"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VHXGiN9SlQ-","executionInfo":{"status":"ok","timestamp":1615608023538,"user_tz":300,"elapsed":22221,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"af6466c2-fb64-4921-b4ff-a1e920a4d4ca"},"source":["countries_percentages = []\n","cities_percentages = []\n","matching_indices = []\n","for i in investigator_indices:\n","  for a in author_indices:\n","    # if these are matching instances\n","    if df_input[8][i] == df_input[8][a]:\n","      matching_indices.append((i-1018,a+1))\n","      count = 0\n","      true_count = 0\n","      # check if all the countries in the investigator array are in the author\n","      # array\n","      for j in countries_arr[i]:\n","        if j in countries_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of countries in this array found in other array to\n","        # array of percentages\n","        countries_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        countries_percentages.append(1)\n","      count = 0\n","      true_count = 0\n","      # check if all the countries in the author array are in the investigator\n","      # array\n","      for k in countries_arr[a]:\n","        if k in countries_arr[i]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        countries_percentages.append(true_count/count)\n","      else:\n","        countries_percentages.append(1)\n","      \n","      count = 0\n","      true_count = 0\n","      # check if all the cities in the investigator array are in the author\n","      # array\n","      for j in cities_arr[i]:\n","        if j in cities_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of cities in this array found in other array to\n","        # array of percentages\n","        cities_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        cities_percentages.append(1)\n","      count = 0\n","      true_count = 0\n","      # check if all the cities in the author array are in the investigator\n","      # array\n","      for k in cities_arr[a]:\n","        if k in cities_arr[i]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        cities_percentages.append(true_count/count)\n","      else:\n","        cities_percentages.append(1)\n","print(\"countries percentages: \" + str(countries_percentages))\n","print(\"cities percentages: \" + str(cities_percentages))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["countries percentages: [1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n","cities percentages: [1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1, 1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vl3Wm_wrm7lx"},"source":["* The output from the above code is two arrays\n","* The first one contains the percentages of how many of the countries in the investigator array are in the author array and vice versa\n","* The second one contains the percentages of how many of the cities in the investigator array are in the author array and vice versa\n","* We can see that both arrays contains only 1s, which means that the countries and cities of matching instances are always the same in the dataset\n","* Let us, using the code below, calculate what we did above for non-matching instances"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1q5Y8BeroZBF","executionInfo":{"status":"ok","timestamp":1615608038996,"user_tz":300,"elapsed":37665,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"4215eb10-607a-4ed0-f040-69a2844b2aa5"},"source":["countries_percentages = []\n","cities_percentages = []\n","for i in investigator_indices:\n","  for a in author_indices:\n","    # if these are not matching instances\n","    if (i,a) not in matching_indices:\n","      count = 0\n","      true_count = 0\n","      # check if all the countries in the investigator array are in the author\n","      # array\n","      for j in countries_arr[i]:\n","        if j in countries_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of countries in this array found in other array to\n","        # array of percentages\n","        countries_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        countries_percentages.append(1)\n","      count = 0\n","      true_count = 0\n","      # check if all the countries in the author array are in the investigator\n","      # array\n","      for k in countries_arr[a]:\n","        if k in countries_arr[i]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        countries_percentages.append(true_count/count)\n","      else:\n","        countries_percentages.append(1)\n","      \n","      count = 0\n","      true_count = 0\n","      # check if all the cities in the investigator array are in the author\n","      # array\n","      for j in cities_arr[i]:\n","        if j in cities_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of cities in this array found in other array to\n","        # array of percentages\n","        cities_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        cities_percentages.append(1)\n","      count = 0\n","      true_count = 0\n","      # check if all the cities in the author array are in the investigator\n","      # array\n","      for k in cities_arr[a]:\n","        if k in cities_arr[i]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        cities_percentages.append(true_count/count)\n","      else:\n","        cities_percentages.append(1)\n","print(\"average countries percentage: \" + str(sum(countries_percentages)/len(countries_percentages)))\n","print(\"std dev of countries percentages: \" + str(stat.stdev(countries_percentages)))\n","print(\"average cities percentages: \" + str(sum(cities_percentages)/len(cities_percentages)))\n","print(\"std dev of cities percentages: \" + str(stat.stdev(cities_percentages)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["average countries percentage: 0.5301477787654706\n","std dev of countries percentages: 0.49604086269602243\n","average cities percentages: 0.5006816560075751\n","std dev of cities percentages: 0.49960687762197653\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HvTkDynV0Bo7"},"source":["* The first two values are the average and standard deviation, respectively, of the percentages of how many of the countries in the investigator array are in the author array and vice versa\n","* The last two values are the average and standard deviation, respectively, of the percentages of how many of the cities in the investigator array are in the author array and vice versa\n","* The values indicate that, typically, either the countries and cities exactly match or they do not match at all\n","* Furthermore, this means that for both cities and countries, matching instances have roughly 2x as high of a matching percentage\n","* Moving forward, coauthors/coinvestigators seem to be useless as features\n","* Simply looking at the data shows us that matching instances do not seem to have the same coauthors/coinvestigators\n","* Looking at the data for topics in matching instances is a different story\n","* Topics in matching instances do seem to show some small overlap oftentimes\n","* One intuition is to create a dictionary which records each pair of distinct topics and the number of times the pair appears together, so that it becomes clear which topics are likely to be related\n","* However, this quickly becomes intractable\n","* Let us look at the code below to see how many distinct topics there are in the authors dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0euOftoSnxgq","executionInfo":{"status":"ok","timestamp":1615608039268,"user_tz":300,"elapsed":37925,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"70677ec0-8597-4b40-cdaa-ff0aa25c842d"},"source":["distinct_topics = []\n","for i in author_indices:\n","  for j in topics_arr[i]:\n","    if j not in distinct_topics:\n","      distinct_topics.append(j)\n","print(\"The number of distinct topics in the author dataset is: \" + \n","      str(len(distinct_topics)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The number of distinct topics in the author dataset is: 4581\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DwCATEFz7tas"},"source":["* The code above tells us that there are 4581 distinct topics in the author dataset\n","* This means that the dictionary mentioned above would have roughly 21 million elements\n","* The code below shows how such a dictionary could be constructed, but running it would take 5+ hours"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"NtZc4jqL7uRy","executionInfo":{"status":"ok","timestamp":1615608039271,"user_tz":300,"elapsed":37912,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"20a8b474-f9cf-4646-fe46-47bd16a45414"},"source":["\"\"\"\n","topic_topic_comp = {}\n","for i in distinct_topics:\n","  for j in distinct_topics:\n","    topic_topic_comp[(i,j)] = 0\n","count = 0\n","for i in topic_topic_comp:\n","  print(count)\n","  for j in topics_arr:\n","    topic_1, topic_2 = i\n","    if topic_1 in j and topic_2 in j:\n","      curr_count = topic_topic_comp[i]\n","      topic_topic_comp[i] = curr_count + 1\n","  count += 1\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\ntopic_topic_comp = {}\\nfor i in distinct_topics:\\n  for j in distinct_topics:\\n    topic_topic_comp[(i,j)] = 0\\ncount = 0\\nfor i in topic_topic_comp:\\n  print(count)\\n  for j in topics_arr:\\n    topic_1, topic_2 = i\\n    if topic_1 in j and topic_2 in j:\\n      curr_count = topic_topic_comp[i]\\n      topic_topic_comp[i] = curr_count + 1\\n  count += 1\\n'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"KUjSRIPs8zTT"},"source":["* Clearly, we need a simpler solution for determining the similarity of topics in different instances\n","* One such method is to simply take note of how many topics from the investigator instance are present in the author instance\n","* Let us determine how important this is in matching instances\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeO2k2YCBDn6","executionInfo":{"status":"ok","timestamp":1615608049441,"user_tz":300,"elapsed":48067,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"02369a54-825e-42fb-aebe-5d299e5a1995"},"source":["topics_percentages = []\n","for i in investigator_indices:\n","  for a in author_indices:\n","    # if these are matching instances\n","    if df_input[8][i] == df_input[8][a]:\n","      count = 0\n","      true_count = 0\n","      # check if all the topics in the investigator array are in the author\n","      # array\n","      for j in topics_arr[i]:\n","        if j in topics_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of topics in this array found in other array to\n","        # array of percentages\n","        topics_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        topics_percentages.append(1)\n","print(\"topics percentages: \" + str(topics_percentages))\n","print(\"mean topics percentage: \" + str(sum(topics_percentages)/len(topics_percentages)))\n","print(\"std dev of topics percentages: \" + str(stat.stdev(topics_percentages)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["topics percentages: [1.0, 0.14285714285714285, 0.16666666666666666, 0.8333333333333334, 0.5, 0.75, 0.0, 0.5, 0.16666666666666666, 0.6, 0.0, 1.0, 0.2, 0.0, 1.0, 0.5, 0.75, 0.75, 0.3333333333333333, 0.25, 0.75, 1.0, 0.25, 0.0, 1.0, 1.0, 0.0, 0.8, 0.0, 1.0, 1, 1.0, 0.42857142857142855, 0.4, 1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 0.4444444444444444, 1.0, 1.0, 0.5, 0.3333333333333333, 0.2857142857142857, 0.7, 0.25, 0.3333333333333333, 0.3333333333333333, 0.0, 0.2, 0.75, 1.0, 0.5, 0.0, 1.0, 0.2857142857142857, 0.6, 1.0, 0.7777777777777778, 0.5, 1.0, 0.75, 0.3333333333333333, 0.1875, 0.3333333333333333, 1.0, 0.16666666666666666, 0.0, 1.0, 0.6363636363636364, 0.5, 0.16666666666666666, 0.3333333333333333, 1.0, 1.0, 0.5, 0.6666666666666666, 0.5, 0.75, 1.0, 1.0, 0.0, 1.0, 0.0, 0.75, 1.0, 0.0, 1.0, 0.7142857142857143, 1.0, 0.42857142857142855, 1.0, 0.0, 0.2857142857142857, 1.0, 1.0, 1.0, 0.2222222222222222, 0.625, 0.3333333333333333, 0.5, 0.7, 1.0, 0.6666666666666666, 0.3333333333333333, 0.6, 0.14285714285714285, 1.0, 0.5555555555555556, 1.0, 1.0, 1.0, 0.5, 0.08333333333333333, 0.5, 0.5, 0.75, 0.8571428571428571, 0.16666666666666666, 1.0, 0.5, 0.25, 0.8, 0.5, 0.5, 0.5, 1.0, 0.8, 1.0, 0.1111111111111111, 0.2857142857142857, 1.0, 1.0, 0.23076923076923078, 0.16666666666666666, 1.0, 1.0, 0.6666666666666666, 1.0, 1.0, 0.75, 1.0, 0.6666666666666666, 0.6, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.5, 0.25, 0.5, 1.0, 1.0, 0.75, 0.2, 1.0, 0.5, 1.0, 0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.25, 0.6666666666666666, 0.25, 1.0, 1.0, 1.0, 0.5, 0.0, 0.5, 0.5, 1.0, 0.8333333333333334, 0.2222222222222222, 0.0, 0.75, 0.8, 0.5, 0.5, 0.6, 1, 1.0, 1.0, 0.9166666666666666, 0.5714285714285714, 0.5, 1.0, 0.5714285714285714, 0.5, 0.4, 0.3333333333333333, 0.08333333333333333, 0.0, 1.0, 0.5, 1.0, 0.25]\n","mean topics percentage: 0.6088817129472469\n","std dev of topics percentages: 0.3536576509817388\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CJGufYCvDJb1"},"source":["* We can see from the output above that, on average, for matching instances, the topics of the authors instance contain roughly 60% of the topics of the investigators instance\n","* There is significant variability\n","* Let us now compare this with the average for non-matching instances"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Td6HNXmtFeKc","executionInfo":{"status":"ok","timestamp":1615608060317,"user_tz":300,"elapsed":58932,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"f615a454-92e4-4e60-9809-c833d9672399"},"source":["topics_percentages = []\n","for i in investigator_indices:\n","  for a in author_indices:\n","    if (i,a) not in matching_indices:\n","      count = 0\n","      true_count = 0\n","      # check if all the topics in the investigator array are in the author\n","      # array\n","      for j in topics_arr[i]:\n","        if j in topics_arr[a]:\n","          true_count += 1\n","        count += 1\n","      if count > 0:\n","        # add percentage of topics in this array found in other array to\n","        # array of percentages\n","        topics_percentages.append(true_count/count)\n","      else:\n","        # if the array is empty add 1 to array of percentages \n","        topics_percentages.append(1)\n","print(\"mean topics percentage: \" + str(sum(topics_percentages)/len(topics_percentages)))\n","print(\"std dev of topics percentages: \" + str(stat.stdev(topics_percentages)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean topics percentage: 0.25191698608244434\n","std dev of topics percentages: 0.3304374112237088\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aC1fJPE-HTz3"},"source":["* We can see from the output above that, on average, for non-matching instances, the topics of the authors instance contain roughly 25% of the topics of the investigators instance\n","* There is once again significant variability, but it is around the same as the variability for matching instances\n","* This tells us that matching instances contain, on average, 2.4x as many matching topics as non-matching instances\n","* This tells us that this is a good method of differentiating between non-matching and matching instances\n","* Before constructing the algorithm, let's examine another assumption we are making: not every item from one data source has to have a match in the other data source\n","* This is because if a name in one dataset does not match as specified previously with any other name in the other dataset, then it has no matches\n","* The name requirement is made so stringent (consider it equivalent to increasing distance between the instances to infinity or decreasing similarity to 0) as it applies to all matching instances in the data and it seems quite reasonable for future data\n","* Let us use the code below to see if every instance in the investigator dataset has a match in the author dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSkCBfsFXFev","executionInfo":{"status":"ok","timestamp":1615608060323,"user_tz":300,"elapsed":58927,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"4df2e09b-bc32-4017-aacc-09047eafaa6d"},"source":["# set to true initially\n","glob_match = True\n","for i in investigator_indices:\n","  # set to false initially\n","  match = False\n","  for a in author_indices:\n","    last = True\n","    middle = True\n","    first = True\n","    # if the last names do not exactly match\n","    if not df_input[3][i] == df_input[3][a]:\n","      last = False\n","    # if these are non-NaN values\n","    investigator_first = df_input[1][i]\n","    author_first = df_input[1][a]\n","    if not str(investigator_first) == 'nan' and not str(author_first) == 'nan':\n","      if investigator_first.find('.') >= 0:\n","        investigator_first = investigator_first.replace('.','')\n","      if author_first.find('.') >= 0:\n","        author_first = author_first.replace('.','')\n","      if len(author_first) > 0 and len(investigator_first) > 0:\n","        # if the length of either the investigator or author first name is 1\n","        if len(investigator_first) == 1 or len(author_first) == 1:\n","          # if the first characters of the first names do not match\n","          if not investigator_first[0] == author_first[0]:\n","            first = False\n","        # otherwise\n","        elif len(investigator_first) > 1 and len(author_first) > 1:\n","          # if the first names do not match\n","          if not investigator_first == author_first:\n","            first = False\n","    # if these are non-NaN values\n","    investigator_middle = df_input[2][i]\n","    author_middle = df_input[2][a]\n","    if not str(investigator_middle) == 'nan' and not str(author_middle) == 'nan':\n","      if investigator_middle.find('.') >= 0:\n","        investigator_middle = investigator_middle.replace('.','')\n","      if author_middle.find('.') >= 0:\n","        author_middle = author_middle.replace('.','')\n","      if len(author_middle) > 0 and len(investigator_middle) > 0:\n","        # if the length of either the investigator or author middle name is 1\n","        if len(investigator_middle) == 1 or len(author_middle) == 1:\n","          # if the first characters of the middle names do not match\n","          if not investigator_middle[0] == author_middle[0]:\n","            middle = False\n","        # otherwise\n","        elif len(investigator_middle) > 1 and len(author_middle) > 1:\n","          # if the middle names do not match\n","          if not investigator_middle == author_middle:\n","            middle = False\n","    # if all of the first, middle, or last names match as described, set match\n","    # to true and break from the loop; otherwise, match remains false\n","    if not first or not middle or not last:\n","      pass\n","    else:\n","      match = True\n","      break\n","  # if match is false, print False\n","  if not match:\n","    glob_match = False\n","    break\n","print(glob_match)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b8JqqU64XIzl"},"source":["* The code outputting False tells us that there is at least one instance in the investigator dataset which has no match in the author dataset\n","* From our investigation on the data thus far, we can begin to construct our algorithm for matching instances"]},{"cell_type":"markdown","metadata":{"id":"Zw7-Mf3jQFiQ"},"source":["## Proposed Clustering Algorithm\n","* From our investigation thus far, it sounds like clustering could be used\n","* Let's take a look at an initial attempt at a solution using the k modes clustering algorithm\n","  * The algorithm briefly works as follows:\n","    * The distance between two instances: dist(x,y) = $\\sum_{i=1}^{d} \\delta_i $  \n","    $\\delta_i = 1 $ if $x_i \\neq y_i $  \n","    $\\delta_i = 0 $ if $x_i = y_i $\n","    1. Randomly select k instances as cluster centers\n","    2. align the instances to their closest cluster centers to form the clusters\n","    3. calculate the modes to get the updated cluster centers\n","    4. repeat steps 2 and 3 until the cluster centers do not change\n","    5. repeat all steps with new randomly selected cluster centers and select the clustering with the lowest total variation within each cluster\n","* We could modify the distance metric somewhat to incorporate what we have learned from exploring the data\n","* Our original intuition is that we can set k to n/2, where n is the number of instances in the dataset\n","* However, this is not guaranteed to create matching pairs, as we can see using the figure below:\n","<!-- $=$ -->"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"SCAaVUgMI7Br","executionInfo":{"status":"ok","timestamp":1615608060326,"user_tz":300,"elapsed":58917,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"afd55736-464e-4195-8e37-a7c30181a110"},"source":["x = [1, 8, 10, 11]\n","y = [0, 0, 0, 0]\n","plt.plot(x, y, 'o', color='black')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fd866ab55d0>]"]},"metadata":{"tags":[]},"execution_count":12},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"6Qz0AN6iPGwZ"},"source":["* If k were set to 2 in the above example, the 3 rightmost points would be in one cluster and the leftmost point would be in its own cluster\n","* Therefore, what we need are clusters of fixed size not a fixed number of clusters\n","* We will draw some inspiration from the k modes algorithm to design our own clustering algorithm which creates clusters of 2 instances\n","* For the similarity metric, we must consider how much weight to give to names, countries, cities, and topics\n","* Because the score from countries, cities, and topics falls in the same scale (0-1), the scale need not be changed\n","* We will give topics 1.2x as much weight as countries and cities because:\n","  * As calculated above, topics tells us that matching instances, on average, have a value 2.4x as as high as non-matching instances while cities and countries tell us that matching instances, on average, have a value 2x as high as non-matching instances\n","  * This means that topics is (2.4/2)=1.2x as informative as countries or cities\n","* Furthermore, in addition to removing from consideration those instances which have no instances in the other dataset with matching names, we will also calculate a score for names\n","* Somewhat counterintuitively, middle names are more informative than first names (because fewer people have them), so these will be given a higher weight\n","* As a heuristic, which could later be improved:\n","  * If the first names are both longer than 1 character and match exactly, the name score is incremented by 0.4\n","  * If one of them is only one character and they both match, the name score is incremented by 0.2\n","  * If the middle names are both longer than 1 character and match exactly, the name score is incremented by 0.6\n","  * If one of them is only one character and they both match, the name score is incremented by 0.3\n","* This proposed algorithm briefly works as follows:\n","  * The similarity between two instances: sim(x,y) = $\\delta_m + \\delta_n + \\frac{1}{2}(\\frac{1}{c_a}\\sum_{j=1}^{c_a}f_{j,x,c} + \\frac{1}{c_i}\\sum_{j=1}^{c_i}f_{j,y,c}) + \\frac{1}{2}(\\frac{1}{u_a}\\sum_{j=1}^{u_a}f_{j,x,u} + \\frac{1}{u_i}\\sum_{j=1}^{u_i}f_{j,y,u}) + 1.2(\\frac{1}{t_i}\\sum_{j=1}^{t_i}f_{j,y,t})$  \n","$f_{j,y,e} = 1 $ if $x_{e_j} \\in y_e$, $f_{j,y,e} = 0 $ if $x_{e_j} \\notin y_e$  \n","$f_{j,x,e} = 1 $ if $y_{e_j} \\in x_e$, $f_{j,x,e} = 0 $ if $y_{e_j} \\notin x_e$  \n","$\\delta_e = 0.6$ if $e = m$ and $x_e = y_e$, $\\delta_e = 0.3$ if $e = m$ and $x_e[0] = y_e[0]$  \n","$\\delta_e = 0.4$ if $e = n$ and $x_e = y_e$, $\\delta_e = 0.2$ if $e = n$ and $x_e[0] = y_e[0]$  \n","  * e in the equation above represents the different features of x and y\n","    * e = {m, n, c, u, t} = {middle name, first name, list of cities, list of cities, list of topics}\n","  * The similarity between two instances is set to 0 if the names explicitly do not match (meaning these instances cannot be in one cluster)\n","  * If an instance has 0 similarity with all other instances, it is removed from consideration\n","  1. Randomly select a point from one data source and pick the point with the highest similarity to it from the other data source as its cluster\n","  2. Remove the point and its most similar point from consideration\n","  3. Repeat steps 1 and 2 until there are no points remaining\n","  4. Calculate total variation among the clusters' similarity scores and keep an array with the clusters\n","  5. Repeat steps 1 through 4 a fixed number of times to get the clustering with the minimum total variation among the clusters' similarity scores\n","* The code below implements the algorithm so that it is not dependent on the previous cells of code (excluding the code cell where the filepaths of the csv files are specified)\n","* The fixed number of times to run the algorithm to find the best matches is currently set to 5 using the \"runs\" variable"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ogeh-ygCUAdi","executionInfo":{"status":"ok","timestamp":1615608195164,"user_tz":300,"elapsed":193741,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"7a85adf0-1bd2-4216-e8d3-1614d06e8210"},"source":["\"\"\"\n","Importing data from CSV.\n","\"\"\"\n","\n","# investigators csv read into pandas dataframe\n","df_investigators = pd.read_csv(investigator_path, \n","                               encoding='latin-1')\n","df_investigators.columns = range(df_investigators.shape[1])\n","inv_ids = []\n","# authors csv read into pandas dataframe\n","df_authors = pd.read_csv(author_path, encoding='latin-1')\n","df_authors.columns = range(df_authors.shape[1])\n","# df_input is concatenation of investigators data to authors data\n","df_input = pd.concat([df_authors, df_investigators], axis=0)\n","df_input = df_input.reset_index()\n","# creating 2d list of coauthor and coinvestigator last names\n","co_names_arr = []\n","for i in df_input[4]:\n","  line_arr = []\n","  if i.find('\\\"') < 0:\n","    co_names_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('\\\"')\n","    while start_ind >= 0:\n","      end_ind = i.find('\\\"', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind+1])\n","      start_ind = i.find('\\\"', end_ind+1)\n","  co_names_arr.append(line_arr)\n","# creating 2d list of topics\n","topics_arr = []\n","for i in df_input[5]:\n","  line_arr = []\n","  if i.find('\\\"') < 0:\n","    topics_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('\\\"')\n","    while start_ind >= 0:\n","      end_ind = i.find('\\\"', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind+1])\n","      start_ind = i.find('\\\"', end_ind+1)\n","  topics_arr.append(line_arr)\n","# creating 2d list of cities\n","cities_arr = []\n","for i in df_input[6]:\n","  line_arr = []\n","  if i[i.find('identifier')+12] == 'n':\n","    cities_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('identifier')+12\n","    while start_ind >= 12:\n","      end_ind = i.find(',', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind])\n","      start_ind = i.find('identifier', end_ind+1)+12\n","  cities_arr.append(line_arr)\n","# creating 2d list of countries\n","countries_arr = []\n","for i in df_input[7]:\n","  line_arr = []\n","  if i[i.find('identifier')+12] == 'n':\n","    countries_arr.append(line_arr)\n","    continue\n","  else:\n","    start_ind = i.find('identifier')+12\n","    while start_ind >= 12:\n","      end_ind = i.find(',', start_ind+1)\n","      line_arr.append(i[start_ind: end_ind])\n","      start_ind = i.find('identifier', end_ind+1)+12\n","  countries_arr.append(line_arr)\n","\n","\"\"\"\n","Recording which indices belong to the author dataset and which belong to the\n","investigator dataset.\n","\"\"\"\n","\n","investigator_indices = []\n","author_indices = []\n","for i in range(len(df_input[0])):\n","  # the rows in the dataframe which correspond to author data\n","  if i < 1019:\n","    author_indices.append(i)\n","  # the rows in the dataframe which correspond to investigator data\n","  else:\n","    investigator_indices.append(i)\n","\n","\"\"\"\n","Algorithm begins here.\n","\"\"\"\n","\n","def pairwise_clustering():\n","  matches = {}\n","  # copy investigator and author indices\n","  c_investigator_indices = investigator_indices.copy()\n","  c_author_indices = author_indices.copy()\n","  while(len(c_investigator_indices) > 0):\n","    index = random.randint(0, len(c_investigator_indices)-1)\n","    # random investigator instance chosen\n","    i = c_investigator_indices[index]\n","    # investigator instance removed so that it can not be used in future clusters\n","    c_investigator_indices.remove(i)\n","    name_match = False\n","    sim_scores = {}\n","    rejects = []\n","    # compare investigator instance to all author instances\n","    for a in c_author_indices:\n","      country_score = 0\n","      city_score = 0\n","      topic_score = 0\n","      name_score = 0\n","      last = True\n","      middle = True\n","      first = True\n","      # if the last names do not exactly match\n","      if df_input[3][i] != df_input[3][a]:\n","        last = False\n","      # if these are non-NaN values\n","      investigator_first = df_input[1][i]\n","      author_first = df_input[1][a]\n","      if str(investigator_first) != 'nan' and str(author_first) != 'nan':\n","        # remove periods\n","        if investigator_first.find('.') >= 0:\n","          investigator_first = investigator_first.replace('.','')\n","        if author_first.find('.') >= 0:\n","          author_first = author_first.replace('.','')\n","        if len(author_first) > 0 and len(investigator_first) > 0:\n","          # if the length of either the investigator or author first name is 1\n","          if len(investigator_first) == 1 or len(author_first) == 1:\n","            # if the first characters of the first names do not match\n","            if not investigator_first[0] == author_first[0]:\n","              first = False\n","            else:\n","              # if the first characters of the first names match\n","              name_score += 0.2\n","          # otherwise\n","          elif len(investigator_first) > 1 and len(author_first) > 1:\n","            # if the first names do not match\n","            if not investigator_first == author_first:\n","              first = False\n","            else:\n","              # if the first names match\n","              name_score += 0.4\n","      # if these are non-NaN values\n","      investigator_middle = df_input[2][i]\n","      author_middle = df_input[2][a]\n","      if str(investigator_middle) != 'nan' and str(author_middle) != 'nan':\n","        # remove periods\n","        if investigator_middle.find('.') >= 0:\n","          investigator_middle = investigator_middle.replace('.','')\n","        if author_middle.find('.') >= 0:\n","          author_middle = author_middle.replace('.','')\n","        if len(author_middle) > 0 and len(investigator_middle) > 0:\n","          # if the length of either the investigator or author middle name is 1\n","          if len(investigator_middle) == 1 or len(author_middle) == 1:\n","            # if the first characters of the middle names do not match\n","            if investigator_middle[0] != author_middle[0]:\n","              middle = False\n","            # if the first characters of the middle names match\n","            else:\n","              name_score += 0.3\n","          # otherwise\n","          elif len(investigator_middle) > 1 and len(author_middle) > 1:\n","            # if the middle names do not match\n","            if investigator_middle != author_middle:\n","              middle = False\n","            # if the middle names match\n","            else:\n","              name_score += 0.6\n","      # if any of the first, middle, or last names do not match as described,\n","      # reject these author instances\n","      if not first or not middle or not last:\n","        rejects.append(a)\n","      else:\n","        name_match = True\n","    # if this investigator instance has no author instances which have matching\n","    # names, then the investigator instance is discarded from consideration for\n","    # clustering\n","    if len(rejects) == len(c_author_indices):\n","      continue\n","    else:\n","      for a in c_author_indices:\n","        # we now compare the topics, cities, and countries lists between the\n","        # investigator instance and non-rejected author instances\n","        if a not in rejects:\n","          count = 0\n","          true_count = 0\n","          # check the percentage of countries found in the investigator list that\n","          # are present in the author list\n","          for j in countries_arr[i]:\n","            if j in countries_arr[a]:\n","              true_count += 1\n","            count += 1\n","          if count > 0:\n","            country_score = true_count/count\n","          else:\n","            # if the array is empty, percentage = 100%\n","            country_score = 1\n","          count = 0\n","          true_count = 0\n","          # check the percentage of countries found in the author list that are\n","          # present in the investigator list\n","          for k in countries_arr[a]:\n","            if k in countries_arr[i]:\n","              true_count += 1\n","            count += 1\n","          # the percentages are averaged here\n","          if count > 0:\n","            country_score = (country_score+(true_count/count))/2\n","          else:\n","            country_score = (country_score+1)/2\n","          \n","          count = 0\n","          true_count = 0\n","          # calculate the average percent of cities present in both the author\n","          # and investigator instances\n","          for j in cities_arr[i]:\n","            if j in cities_arr[a]:\n","              true_count += 1\n","            count += 1\n","          if count > 0:\n","            city_score = true_count/count\n","          else:\n","            city_score = 1\n","          count = 0\n","          true_count = 0\n","          for k in cities_arr[a]:\n","            if k in cities_arr[i]:\n","              true_count += 1\n","            count += 1\n","          if count > 0:\n","            city_score = (city_score+(true_count/count))/2\n","          else:\n","            city_score = (city_score+1)/2\n","          \n","          count = 0\n","          true_count = 0\n","          # check what percentage of the topics in the investigator instance are\n","          # in the author instance\n","          for j in topics_arr[i]:\n","            if j in topics_arr[a]:\n","              true_count += 1\n","            count += 1\n","          if count > 0:\n","            topic_score = true_count/count\n","          else:\n","            topic_score = 1\n","          # total score calculated\n","          sim_scores[a] = name_score+country_score+city_score+1.2*topic_score\n","    # the author instance with the maximum similarity to the investigator instance\n","    # is selected\n","    max_score_index = -1\n","    max_score = 0\n","    for s in sim_scores:\n","      curr_score = sim_scores[s]\n","      if curr_score > max_score:\n","        max_score = curr_score\n","        max_score_index = s\n","    if max_score_index == -1:\n","      continue\n","    matches[(i-1018,max_score_index+1)] = max_score\n","    # the matched author instance is then removed from consideration for future\n","    # clusters\n","    c_author_indices.remove(max_score_index)\n","  # the standard deviation in similarity scores for all the matches is calculated\n","  all_sims = []\n","  only_matches = []\n","  for m in matches:\n","    all_sims.append(matches[m])\n","    only_matches.append(m)\n","  only_matches = tuple(only_matches)\n","  if len(all_sims) == 1:\n","    all_matches[only_matches] = 0\n","  else:\n","    all_matches[only_matches] = stat.stdev(all_sims)\n","\n","all_matches = {}\n","# number of times to run algorithm to get best clustering is set here\n","runs = 5\n","threads = []\n","# creates a thread to call create a clustering for each iteration\n","# should speed up by approx. a factor of the number of cores\n","for run in range(runs):\n","  t = Thread(target=pairwise_clustering, args=())\n","  t.start()\n","  threads.append(t)\n","for t in threads:\n","  t.join()\n","\n","# the clustering with the least standard deviation is selected as the best clustering\n","# of matching instances\n","least_stdev = 1000\n","best_matches = []\n","for element in all_matches:\n","  curr_stdev = all_matches[element]\n","  if curr_stdev < least_stdev:\n","    least_stdev = curr_stdev\n","    best_matches = element\n","print(\"ids of matching instances:\")\n","for b in best_matches:\n","  print(b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["ids of matching instances:\n","(221, 230)\n","(28, 31)\n","(286, 327)\n","(518, 621)\n","(84, 92)\n","(567, 686)\n","(246, 849)\n","(561, 679)\n","(440, 507)\n","(531, 639)\n","(451, 530)\n","(277, 312)\n","(171, 171)\n","(629, 753)\n","(755, 892)\n","(713, 850)\n","(670, 806)\n","(594, 720)\n","(377, 433)\n","(17, 18)\n","(397, 910)\n","(336, 377)\n","(520, 624)\n","(598, 728)\n","(285, 326)\n","(129, 144)\n","(160, 164)\n","(312, 356)\n","(555, 670)\n","(41, 345)\n","(791, 939)\n","(256, 285)\n","(270, 304)\n","(232, 252)\n","(29, 32)\n","(819, 969)\n","(602, 730)\n","(355, 404)\n","(39, 45)\n","(59, 65)\n","(612, 742)\n","(298, 342)\n","(496, 460)\n","(201, 617)\n","(587, 706)\n","(623, 746)\n","(245, 266)\n","(787, 538)\n","(343, 386)\n","(674, 810)\n","(204, 202)\n","(64, 68)\n","(389, 454)\n","(410, 473)\n","(46, 51)\n","(335, 376)\n","(553, 369)\n","(407, 470)\n","(836, 1018)\n","(112, 119)\n","(158, 162)\n","(111, 118)\n","(423, 486)\n","(142, 682)\n","(537, 647)\n","(275, 310)\n","(795, 943)\n","(269, 302)\n","(798, 947)\n","(793, 941)\n","(672, 441)\n","(724, 871)\n","(317, 361)\n","(736, 882)\n","(157, 159)\n","(754, 891)\n","(462, 556)\n","(481, 577)\n","(19, 19)\n","(711, 410)\n","(470, 566)\n","(213, 215)\n","(533, 642)\n","(718, 854)\n","(144, 143)\n","(688, 825)\n","(439, 506)\n","(665, 800)\n","(259, 286)\n","(179, 177)\n","(192, 188)\n","(303, 346)\n","(80, 90)\n","(86, 95)\n","(14, 13)\n","(775, 911)\n","(315, 359)\n","(237, 256)\n","(5, 2)\n","(624, 747)\n","(222, 234)\n","(762, 899)\n","(278, 313)\n","(778, 913)\n","(85, 93)\n","(776, 4)\n","(511, 608)\n","(52, 58)\n","(464, 557)\n","(265, 299)\n","(381, 436)\n","(354, 403)\n","(597, 727)\n","(346, 390)\n","(690, 828)\n","(710, 848)\n","(342, 288)\n","(392, 455)\n","(549, 661)\n","(653, 787)\n","(51, 57)\n","(131, 130)\n","(717, 853)\n","(420, 484)\n","(81, 91)\n","(829, 994)\n","(494, 588)\n","(823, 982)\n","(430, 494)\n","(7, 5)\n","(90, 102)\n","(749, 889)\n","(627, 751)\n","(482, 578)\n","(638, 766)\n","(305, 348)\n","(601, 729)\n","(337, 930)\n","(700, 841)\n","(78, 89)\n","(510, 606)\n","(645, 777)\n","(705, 495)\n","(139, 104)\n","(165, 166)\n","(62, 67)\n","(161, 165)\n","(22, 22)\n","(45, 48)\n","(816, 960)\n","(527, 632)\n","(545, 656)\n","(215, 219)\n","(228, 243)\n","(394, 760)\n","(220, 229)\n","(429, 492)\n","(663, 798)\n","(281, 318)\n","(666, 801)\n","(280, 315)\n","(820, 970)\n","(596, 726)\n","(375, 431)\n","(94, 105)\n","(35, 38)\n","(236, 255)\n","(366, 894)\n","(740, 885)\n","(539, 561)\n","(689, 826)\n","(250, 275)\n","(169, 169)\n","(416, 481)\n","(576, 693)\n","(309, 353)\n","(772, 907)\n","(625, 748)\n","(266, 300)\n","(453, 532)\n","(130, 129)\n","(461, 120)\n","(483, 579)\n","(188, 187)\n","(681, 815)\n","(692, 829)\n","(234, 253)\n","(98, 107)\n","(49, 55)\n","(67, 71)\n","(401, 464)\n","(413, 477)\n","(460, 548)\n","(455, 534)\n","(362, 414)\n","(262, 293)\n","(568, 687)\n","(454, 533)\n","(437, 505)\n","(685, 822)\n","(148, 148)\n","(792, 940)\n","(242, 263)\n","(239, 257)\n","(89, 99)\n","(694, 836)\n","(53, 59)\n","(631, 757)\n","(699, 840)\n","(173, 173)\n","(71, 79)\n","(291, 333)\n","(415, 480)\n","(728, 874)\n","(347, 392)\n","(405, 469)\n","(504, 596)\n","(16, 17)\n","(332, 373)\n","(588, 707)\n","(393, 456)\n","(183, 178)\n","(380, 435)\n","(56, 61)\n","(327, 565)\n","(202, 196)\n","(294, 337)\n","(704, 843)\n","(620, 745)\n","(543, 651)\n","(227, 242)\n","(673, 809)\n","(706, 844)\n","(577, 694)\n","(813, 956)\n","(536, 644)\n","(562, 680)\n","(172, 172)\n","(530, 638)\n","(311, 355)\n","(658, 791)\n","(100, 108)\n","(378, 434)\n","(132, 131)\n","(815, 959)\n","(166, 168)\n","(498, 591)\n","(136, 501)\n","(251, 276)\n","(827, 990)\n","(637, 765)\n","(777, 912)\n","(591, 710)\n","(680, 814)\n","(424, 487)\n","(321, 362)\n","(630, 754)\n","(677, 344)\n","(368, 421)\n","(403, 467)\n","(508, 603)\n","(626, 749)\n","(23, 23)\n","(572, 689)\n","(187, 185)\n","(644, 773)\n","(40, 46)\n","(364, 417)\n","(74, 81)\n","(344, 388)\n","(307, 351)\n","(618, 743)\n","(373, 429)\n","(235, 254)\n","(794, 942)\n","(66, 247)\n","(799, 635)\n","(297, 340)\n","(589, 672)\n","(703, 842)\n","(559, 678)\n","(229, 246)\n","(63, 306)\n","(649, 782)\n","(607, 737)\n","(210, 214)\n","(671, 807)\n","(363, 415)\n","(544, 652)\n","(731, 878)\n","(606, 736)\n","(356, 406)\n","(765, 901)\n","(730, 877)\n","(404, 468)\n","(720, 193)\n","(528, 636)\n","(817, 961)\n","(263, 297)\n","(383, 439)\n","(574, 692)\n","(759, 898)\n","(808, 952)\n","(565, 683)\n","(828, 991)\n","(50, 56)\n","(264, 298)\n","(519, 623)\n","(667, 802)\n","(105, 112)\n","(87, 96)\n","(30, 33)\n","(493, 587)\n","(11, 6)\n","(331, 372)\n","(628, 752)\n","(217, 425)\n","(632, 758)\n","(810, 953)\n","(77, 88)\n","(274, 308)\n","(492, 583)\n","(247, 267)\n","(763, 900)\n","(292, 334)\n","(771, 906)\n","(369, 422)\n","(444, 524)\n","(283, 324)\n","(357, 407)\n","(224, 240)\n","(37, 40)\n","(47, 53)\n","(95, 106)\n","(547, 660)\n","(521, 625)\n","(551, 667)\n","(109, 117)\n","(422, 485)\n","(15, 14)\n","(744, 887)\n","(752, 890)\n","(526, 630)\n","(170, 170)\n","(159, 163)\n","(134, 1014)\n","(770, 905)\n","(650, 783)\n","(729, 875)\n","(133, 133)\n","(431, 497)\n","(479, 575)\n","(306, 350)\n","(58, 64)\n","(13, 12)\n","(643, 770)\n","(32, 35)\n","(382, 437)\n","(223, 237)\n","(485, 580)\n","(592, 713)\n","(719, 857)\n","(812, 955)\n","(830, 996)\n","(781, 918)\n","(784, 545)\n","(698, 839)\n","(745, 888)\n","(683, 820)\n","(515, 619)\n","(499, 592)\n","(150, 152)\n","(790, 935)\n","(534, 643)\n","(27, 30)\n","(70, 78)\n","(661, 797)\n","(65, 70)\n","(288, 329)\n","(38, 42)\n","(113, 121)\n","(361, 411)\n","(88, 97)\n","(68, 72)\n","(797, 945)\n","(273, 307)\n","(768, 904)\n","(516, 620)\n","(295, 338)\n","(249, 273)\n","(289, 331)\n","(833, 1003)\n","(44, 47)\n","(735, 880)\n","(722, 863)\n","(801, 949)\n","(584, 703)\n","(153, 158)\n","(452, 531)\n","(581, 700)\n","(646, 778)\n","(604, 733)\n","(91, 103)\n","(475, 571)\n","(206, 204)\n","(316, 360)\n","(550, 665)\n","(414, 478)\n","(339, 379)\n","(788, 933)\n","(490, 582)\n","(178, 176)\n","(608, 738)\n","(780, 915)\n","(400, 463)\n","(186, 184)\n","(585, 705)\n","(503, 595)\n","(374, 430)\n","(513, 535)\n","(399, 239)\n","(595, 722)\n","(219, 226)\n","(72, 80)\n","(409, 472)\n","(835, 1006)\n","(349, 396)\n","(371, 423)\n","(61, 66)\n","(524, 627)\n","(529, 637)\n","(388, 453)\n","(218, 222)\n","(260, 292)\n","(687, 824)\n","(505, 597)\n","(582, 701)\n","(225, 241)\n","(779, 914)\n","(648, 781)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5dClv-qYMfPD"},"source":["* The code in the cell above outputs the ids of the matching instances and takes around 2 minutes to run (with 5 set as the number of iterations)\n","* The output of the algorithm is non-deterministic (it is not guaranteed to remain the same each time given the same input)\n","* Let us now assess the accuracy of our algorithm by calculating the percentage of ids of actual matching instances which are present in the matching instances our algorithm outputs"]},{"cell_type":"markdown","metadata":{"id":"KWSpy9PNzrJJ"},"source":["## Conclusion"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mAFAqDjvUOJg","executionInfo":{"status":"ok","timestamp":1615608195167,"user_tz":300,"elapsed":193733,"user":{"displayName":"Ravi Parashar","photoUrl":"https://lh4.googleusercontent.com/-6V6e_fJ8H-Y/AAAAAAAAAAI/AAAAAAAAAI8/oxTQSZ1Axok/s64/photo.jpg","userId":"16177984382719198803"}},"outputId":"2ff9174c-c37a-4fda-a5f9-1762b2182051"},"source":["true_count = 0\n","for m in matching_indices:\n","  if m in best_matches:\n","    true_count += 1\n","print(\"accuracy: \" + str(100*(true_count/len(matching_indices))) + \"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 98.05825242718447%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IPtbnwSbW1gr"},"source":["* Even with a low number of iterations, the algorithm almost always achieves 98%+ accuracy\n","* With a higher number of iterations, this accuracy will likely increase, but so will the run time\n","* The algorithm has a worst-case time complexity of $O(n^4)$\n","* To further speed up the code, certain modifications could be made:\n","  * Numpy object arrays could be used place of Pandas dataframes, as the columnar access time for numpy arrays is typically significantly faster than for Pandas dataframes\n","  * Parallelization could be utlized where applicable\n","    * For example, it could be used when calculating the variation among clusters in different clusterings or when calculating city, country, and topic scores\n","* To possibly further boost accuracy and help the algorithm generalize to new data, the following could be explored:\n","  * The percentage of consecutive matching characters in the first name and middle name could be used instead of the current scheme\n","  * Different weights for the different sub-scores of the total similarity score could be tried using a grid search approach\n","  * All unwanted substrings can be removed from features"]}]}